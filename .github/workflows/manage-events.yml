name: Manage NYC Events

on:
  schedule:
    # Run daily at midnight UTC
    - cron: '0 8 * * *'
  workflow_dispatch:  # Allow manual triggering

# Add permissions needed for the workflow
permissions:
  contents: write  # Give permission to push to the repository

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install all dependencies from the main requirements file
          pip install -r requirements.txt
          # Show installed packages for debugging
          echo "Installed packages:"
          pip list
          
      - name: Run Scrapers and Generate Tweets
        run: |
          # Set PYTHONPATH and run the scraper
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          echo "Running scraper with PYTHONPATH: $PYTHONPATH"
          
          # Create necessary directories
          mkdir -p scraper/data # For combined_events.json, etc.
          
          # Check if we have the right dependencies
          echo "Checking installed packages:"
          pip list
          
          # Run with more verbose output
          echo "Running scrapers..."
          python -m scraper.run_all -v
          
          # Examine output files
          echo "Listing data files:"
          find scraper/ -name "*.json" | xargs ls -la
          
          # Copy combined events to public directory for deployment
          echo "Copying combined events to public directory..."
          mkdir -p public/data
          if [ -f "scraper/data/combined_events.json" ]; then
            cp scraper/data/combined_events.json public/data/events.json
          elif [ -f "data/scrapers/google_calendar_events.json" ]; then
            cp data/scrapers/google_calendar_events.json public/data/events.json
          else
            echo "No events file found to copy"
            exit 1
          fi
          
          # Generate tweets (this will still run to post to Twitter if configured)
          echo "Generating tweets"
          python -m scraper.tweet_generator
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          TWITTER_API_KEY: ${{ secrets.TWITTER_API_KEY }}
          TWITTER_API_SECRET: ${{ secrets.TWITTER_API_SECRET }}
          TWITTER_ACCESS_TOKEN: ${{ secrets.TWITTER_ACCESS_TOKEN }}
          TWITTER_ACCESS_TOKEN_SECRET: ${{ secrets.TWITTER_ACCESS_TOKEN_SECRET }}
          
      - name: Commit and Push Changes
        run: |
          set +e  # Continue on errors
          
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          # Add relevant scraper data files (but not public/data/events.json which is gitignored)
          echo "Adding scraper data files..."
          git add scraper/data/*.json || true
          # Note: public/data/events.json is gitignored to prevent stale data commits
          
          # Specifically add cache files with force if they exist
          if ls scraper/data/cache/*.json 1> /dev/null 2>&1; then
            echo "Adding cache files (with force)..."
            git add -f scraper/data/cache/*.json || true
          fi
          
          # Check if we have changes to commit
          echo "Checking for changes to commit..."
          if git diff --quiet && git diff --staged --quiet; then
            echo "No changes to commit"
            exit 0  # Exit successfully
          else
            # Pull latest changes with rebase strategy to avoid merge commits
            echo "Pulling latest changes before committing..."
            git pull --rebase || true
            
            # Commit and push changes
            git commit -m "Update events data and tweets [skip ci]" && \
            git push && echo "Successfully pushed changes" || {
              echo "Push failed, trying again with force option..."
              # If push fails, try force pushing since this is data-only
              git push --force && echo "Successfully force pushed changes" || echo "All push attempts failed"
            }
            
            # Always exit successfully to prevent workflow failure
            exit 0
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          
      # Deploy to Vercel if configured (always deploy with fresh data)
      - name: Deploy to Vercel
        if: success()
        run: |
          if [ -z "$VERCEL_TOKEN" ]; then
            echo "VERCEL_TOKEN is not set. Skipping Vercel deployment."
            exit 0
          fi
          
          echo "Installing and deploying to Vercel with fresh events data..."
          npx vercel --token="${VERCEL_TOKEN}" --prod --confirm
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
          VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}
          VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }} 