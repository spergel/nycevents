name: Manage NYC Events

on:
  schedule:
    # Run daily at midnight UTC
    - cron: '0 0 * * *'
  workflow_dispatch:  # Allow manual triggering

# Add permissions needed for the workflow
permissions:
  contents: write  # Give permission to push to the repository

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install all dependencies from the main requirements file
          pip install -r scraper/requirements.txt
          # Show installed packages for debugging
          echo "Installed packages:"
          pip list
          
      - name: Run Scrapers and Generate Tweets
        run: |
          # Set PYTHONPATH and run the scraper
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          echo "Running scraper with PYTHONPATH: $PYTHONPATH"
          
          # Create necessary directories
          mkdir -p scraper/data
          mkdir -p scraper/tech/data
          mkdir -p scraper/tech/tweets
          mkdir -p tech/tweets  # Create old directory for backward compatibility
          mkdir -p public/tweets  # Create public tweets directory
          
          # Check if we have the right dependencies
          echo "Checking installed packages:"
          pip list
          
          # Run with more verbose output
          echo "Running scrapers..."
          python -m scraper.tech.run_all -v
          
          # Examine output files
          echo "Listing data files:"
          find scraper/ -name "*.json" | xargs ls -la
          
          # Copy combined events to public directory
          echo "Copying combined events to public directory..."
          mkdir -p public/data
          cp scraper/data/combined_events.json public/data/events.json
          
          # Generate tweets
          echo "Generating tweets"
          python -m scraper.tech.tweet_generator
          
          # Copy tweet files to all needed locations
          echo "Copying tweet files to required locations"
          if [ -d "scraper/tech/tweets" ] && [ "$(ls -A scraper/tech/tweets)" ]; then
            cp -r scraper/tech/tweets/* tech/tweets/ || true
            cp -r scraper/tech/tweets/* public/tweets/ || true
          fi
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          
      - name: Commit and Push Changes
        run: |
          set +e  # Continue on errors
          
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          # Add all relevant data files (ignoring errors for gitignored files)
          echo "Adding data files..."
          git add scraper/data/*.json || true
          git add scraper/tech/data/*.json || true
          git add public/data/events.json || true
          
          # Specifically add cache files with force if they exist
          if ls scraper/tech/data/cache/*.json 1> /dev/null 2>&1; then
            echo "Adding cache files (with force)..."
            git add -f scraper/tech/data/cache/*.json || true
          fi
          
          # Add tweet files if they exist
          if ls scraper/tech/tweets/*.html 1> /dev/null 2>&1; then
            echo "Adding tweet files..."
            git add scraper/tech/tweets/*.html || true
          else
            echo "No tweet files found to add"
          fi
          
          # Add old tweet files location too
          if ls tech/tweets/*.html 1> /dev/null 2>&1; then
            echo "Adding old location tweet files..."
            git add tech/tweets/*.html || true
          fi
          
          # Add public tweets files if they exist
          if ls public/tweets/*.html 1> /dev/null 2>&1; then
            echo "Adding public tweet files..."
            git add public/tweets/*.html || true
          fi
          
          # Check if we have changes to commit
          echo "Checking for changes to commit..."
          if git diff --quiet && git diff --staged --quiet; then
            echo "No changes to commit"
            exit 0  # Exit successfully
          else
            # Pull latest changes with rebase strategy to avoid merge commits
            echo "Pulling latest changes before committing..."
            git pull --rebase || true
            
            # Commit and push changes
            git commit -m "Update events data and tweets [skip ci]" && \
            git push && echo "Successfully pushed changes" || {
              echo "Push failed, trying again with force option..."
              # If push fails, try force pushing since this is data-only
              git push --force && echo "Successfully force pushed changes" || echo "All push attempts failed"
            }
            
            # Always exit successfully to prevent workflow failure
            exit 0
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          
      # Deploy to Vercel if configured
      - name: Deploy to Vercel
        if: success()
        run: |
          if [ -z "$VERCEL_TOKEN" ]; then
            echo "VERCEL_TOKEN is not set. Skipping Vercel deployment."
            exit 0
          fi
          
          echo "Installing and deploying to Vercel..."
          npx vercel --token="${VERCEL_TOKEN}" --prod --confirm
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
          VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}
          VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }} 