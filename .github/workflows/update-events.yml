name: Update NYC Events

on:
  schedule:
    # Run twice a week (Monday and Thursday at 2:00 UTC)
    - cron: '0 2 * * 1,4'
  workflow_dispatch:  # Allow manual triggering

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 pandas lxml tqdm python-dateutil ics

      - name: Verify directory structure and prepare environment
        run: |
          echo "Current working directory: $(pwd)"
          echo "Directory contents:"
          ls -la
          
          # Create necessary directories if they don't exist
          mkdir -p scraper/tech/scrapers
          mkdir -p scraper/data
          mkdir -p public/data
          
          # Copy your local files to the repository if they don't exist
          if [ ! -f "scraper/tech/run_all.py" ]; then
            echo "Creating run_all.py script"
            cat > scraper/tech/run_all.py << 'EOL'
import os
import sys
import json
import logging
import time
import argparse
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)

def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='NYC Events Scraper')
    parser.add_argument('--output', help='Output file path', default='public/data/events.json')
    parser.add_argument('--append', help='Append to existing data', action='store_true')
    args = parser.parse_args()
    
    # Create dummy events data for testing
    events = {
        "events": [
            {
                "id": "test_event_1",
                "name": "Test Tech Event",
                "start_date": datetime.now().isoformat(),
                "end_date": datetime.now().isoformat(),
                "location": "New York, NY",
                "description": "This is a test event generated by the GitHub Actions workflow",
                "url": "https://example.com",
                "categories": ["tech"]
            }
        ]
    }
    
    # Ensure output directory exists
    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    
    # Save events to file
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(events, f, indent=2, ensure_ascii=False)
    
    logging.info(f"Saved {len(events['events'])} events to {args.output}")
    return True

if __name__ == "__main__":
    main()
EOL
          fi
          
          # Create empty __init__.py files in the necessary directories
          touch scraper/__init__.py
          touch scraper/tech/__init__.py
          touch scraper/tech/scrapers/__init__.py
          
      - name: Run event scrapers
        run: |
          # Add the current directory to PYTHONPATH
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          
          # Run the scraper with Python module system
          python -m scraper.tech.run_all --output public/data/events.json
        
      - name: Check for changes
        id: git-check
        run: |
          git add public/data/events.json
          git status --porcelain
          echo "modified=$(if git status --porcelain | grep 'public/data/events.json'; then echo 'true'; else echo 'false'; fi)" >> $GITHUB_OUTPUT

      - name: Commit and push if changed
        if: steps.git-check.outputs.modified == 'true'
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git commit -m "Update events data [skip ci]" public/data/events.json
          git push 